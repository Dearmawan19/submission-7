# -*- coding: utf-8 -*-
"""notebookAkhirproyek.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwPelu4ktMDCmHspkoeZMCn1P4XhMe8N

# Proyek Machine Learning - Dearmawan
# Rekomendasi Buku

# Import Libraries
"""

# Install library yang diperlukan
!pip install pandas numpy==1.24.4 scikit-learn scikit-surprise matplotlib seaborn pandas

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Reader, Dataset, SVD, KNNBasic
from surprise.model_selection import train_test_split as surprise_train_test_split
from surprise import accuracy
import re
# Mengatur opsi tampilan Pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

print("Libraries imported successfully!")

"""# Data Load"""

#Data load
path_books = 'Books.csv'
path_users = 'Users.csv'
path_ratings = 'Ratings.csv'

try:
    books_df = pd.read_csv(path_books, low_memory=False)
    users_df = pd.read_csv(path_users, low_memory=False)
    ratings_df = pd.read_csv(path_ratings, low_memory=False)
except FileNotFoundError:
    print("Pastikan file Books.csv, Users.csv, dan Ratings.csv ada di direktori yang benar atau sesuaikan path.")
print("Shape of books_df:", books_df.shape)
print("Shape of users_df:", users_df.shape)
print("Shape of ratings_df:", ratings_df.shape)

""" # Data Understanding"""

#Book dataset
print("Info books_df:")
books_df.info()
print("\nMissing values in books_df:")
print(books_df.isnull().sum())
print("\nFirst 5 rows of books_df:")
print(books_df.head())
print("\nData types in 'Year-Of-Publication':")
print(books_df['Year-Of-Publication'].unique()[:20]) # Melihat beberapa nilai unik untuk mengecek tipe data

"""Berdasarkan output yang Anda berikan, dataset books_df ini cukup besar dengan lebih dari 270.000 entri buku dan memiliki kelengkapan data yang sangat baik untuk kolom-kolom inti seperti ISBN, Judul Buku, Penulis, dan Penerbit, dengan hanya sedikit sekali nilai yang hilang (masing-masing 2 untuk Penulis dan Penerbit, serta 3 untuk URL gambar besar). Semua kolom saat ini bertipe object, yang mengindikasikan bahwa kolom seperti Year-Of-Publication, meskipun dalam sampel 20 nilai unik pertama tampak seperti tahun (misalnya, '2002', '2001'), perlu dikonversi menjadi tipe numerik untuk analisis lebih lanjut dan mungkin memerlukan pembersihan jika ada entri yang tidak valid di luar sampel tersebut. Kehadiran URL gambar juga memberikan potensi untuk fitur tambahan jika diperlukan, meskipun fokus utama kemungkinan akan pada data tekstual untuk sistem rekomendasi."""

# User dataset
print("Info users_df:")
users_df.info()
print("\nMissing values in users_df:")
print(users_df.isnull().sum()) # Banyak nilai Age yang hilang
print("\nFirst 5 rows of users_df:")
print(users_df.head())
print("\nDescriptive statistics for Age:")
print(users_df['Age'].describe())

"""Dataset users_df berisi informasi mengenai 278.858 pengguna, dengan detail mencakup User-ID (integer unik), Location (teks), dan Age (float). Kolom Age menjadi perhatian utama karena memiliki sejumlah besar nilai yang hilang (110.762 data) dan memerlukan pembersihan data akibat adanya nilai yang tidak realistis seperti usia 0 atau 244 tahun, meskipun data usia yang valid menunjukkan rata-rata sekitar 34.75 tahun dan median 32 tahun. Sementara itu, kolom User-ID dan Location tampak lengkap tanpa ada data yang hilang."""

# ratings dataset
print("Info ratings_df:")
ratings_df.info()
print("\nMissing values in ratings_df:")
print(ratings_df.isnull().sum())
print("\nFirst 5 rows of ratings_df:")
print(ratings_df.head())
print("\nDescriptive statistics for Book-Rating:")
print(ratings_df['Book-Rating'].describe())
print("\nUnique values in Book-Rating:")
print(sorted(ratings_df['Book-Rating'].unique()))

"""Dataset ratings_df sangatlah besar, mencakup lebih dari 1,1 juta entri rating tanpa ada nilai yang hilang pada kolom User-ID, ISBN, maupun Book-Rating. Hal yang paling menonjol dari statistik deskriptif Book-Rating adalah dominasi nilai 0, di mana baik kuartil pertama (25%) maupun median (50%) menunjukkan angka 0. Ini mengindikasikan bahwa sebagian besar entri dalam dataset ini adalah rating implisit atau "tidak ada rating", meskipun skala rating eksplisitnya berkisar dari 1 hingga 10, dengan rata-rata rating keseluruhan sekitar 2.87 dan kuartil ketiga (75%) berada di angka 7."""

#Visualisasi Distribusi Rating Buku
plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=ratings_df, palette='viridis')
plt.title('Distribution of Book Ratings')
plt.xlabel('Book Rating')
plt.ylabel('Count')
plt.show()

"""Visualisasi distribusi rating buku dengan jelas menunjukkan bahwa rating 0 mendominasi secara signifikan, dengan jumlahnya melebihi 700.000, jauh melampaui rating lainnya. Rating eksplisit (1-10) memiliki frekuensi yang jauh lebih rendah, dengan rating 8, 9, dan 10 menjadi yang paling banyak diberikan di antara rating eksplisit tersebut, meskipun jumlahnya masih di bawah 100.000 masing-masing. Ini mengindikasikan bahwa sebagian besar interaksi dalam dataset adalah implisit atau "tidak ada rating", dan untuk analisis yang melibatkan preferensi eksplisit, fokus pada rating 1-10 akan lebih relevan, dengan perhatian khusus pada rating tinggi (7-10) yang menunjukkan sentimen positif yang lebih kuat."""

# Visualisai Distribusi Usia Pengguna
plt.figure(figsize=(10,6))
sns.histplot(users_df['Age'].dropna(), bins=30, kde=True)
plt.title('Distribution of User Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.xlim(0, 100) # Batasi usia agar lebih relevan
plt.show()

"""Berdasarkan histogram distribusi usia pengguna, terlihat bahwa mayoritas pengguna berada dalam rentang usia muda hingga paruh baya, dengan puncaknya di sekitar usia 30-an. Distribusi ini cenderung miring ke kanan (right-skewed), menunjukkan bahwa meskipun ada pengguna di berbagai kelompok usia hingga sekitar 80-an, jumlah mereka menurun drastis setelah usia 40-an, mengindikasikan basis pengguna yang kuat pada demografi yang lebih muda."""

# visualisai Top 10 Buku dengan Ratings Count Terbanyak
# Gabungkan ratings dengan books untuk mendapatkan judul buku
ratings_with_titles = ratings_df.merge(books_df[['ISBN', 'Book-Title']], on='ISBN')

top_10_rated_books = ratings_with_titles['Book-Title'].value_counts().head(10)
plt.figure(figsize=(12, 7))
sns.barplot(x=top_10_rated_books.values, y=top_10_rated_books.index, palette='mako')
plt.title('Top 10 Most Rated Books')
plt.xlabel('Number of Ratings')
plt.ylabel('Book Title')
plt.show()

"""Grafik menunjukkan bahwa buku "Wild Animus" memiliki jumlah rating yang jauh melampaui buku-buku lain dalam daftar 10 teratas, menandakan popularitas atau jumlah interaksi yang sangat tinggi dibandingkan dengan judul-judul lain seperti "The Lovely Bones: A Novel" atau "The Da Vinci Code" yang berada di posisi berikutnya. Meskipun semua buku dalam daftar ini masuk dalam kategori paling banyak diberi rating, ada jurang pemisah yang signifikan antara posisi pertama dengan sembilan buku lainnya, menunjukkan dominasi "Wild Animus" dalam hal jumlah rating yang diterima.

#Data Preparation
"""

# Persiapan data book
# --- Persiapan books_df ---
print("Original unique 'Year-Of-Publication' examples:", books_df['Year-Of-Publication'].unique()[:5])
# Koreksi 'Year-Of-Publication'
books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')
# Isi NaN dengan nilai yang tidak mungkin mempengaruhi analisis, atau drop jika sedikit. Kita isi dengan median tahun yang valid.
median_year = books_df['Year-Of-Publication'].median()
books_df['Year-Of-Publication'].fillna(median_year, inplace=True)
books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(int)

# Isi NaN pada Book-Author dan Publisher
books_df['Book-Author'].fillna('Unknown Author', inplace=True)
books_df['Publisher'].fillna('Unknown Publisher', inplace=True)

# Drop kolom URL gambar karena tidak akan digunakan
books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], inplace=True, errors='ignore')

print("\nInfo books_df after cleaning:")
books_df.info()
print("\nMissing values in books_df after cleaning:")
print(books_df.isnull().sum())

"""Dapat dilihat sudah dilakukan penaganan missing value dan drop kolom yang tidak perlu sehingga data memiliki jumlah yg sama untuk dataset book"""

# Persiaoan dataset users
# --- Persiapan users_df ---
# Menangani outliers dan NaN pada Age
users_df.loc[(users_df['Age'] > 90) | (users_df['Age'] < 5), 'Age'] = np.nan
users_df['Age'].fillna(users_df['Age'].median(), inplace=True)
users_df['Age'] = users_df['Age'].astype(int)

print("\nInfo users_df after cleaning:")
users_df.info()
print("\nMissing values in users_df after cleaning:")
print(users_df.isnull().sum())

"""Dapat dilihat outlier sudah diatasi dan tidak adanya missing value."""

# Persiapan dataset ratings
# --- Persiapan ratings_df ---
# Untuk Content-Based, kita hanya perlu books_df yang sudah bersih.
# Untuk Collaborative Filtering, kita perlu ratings_df dan users_df.

# Filter rating eksplisit untuk Collaborative Filtering
ratings_explicit_df = ratings_df[ratings_df['Book-Rating'] != 0]
print(f"\nOriginal ratings: {len(ratings_df)}, Explicit ratings: {len(ratings_explicit_df)}")

# Gabungkan dengan informasi buku untuk kemudahan
full_data_explicit = ratings_explicit_df.merge(books_df[['ISBN', 'Book-Title']], on='ISBN')
full_data_explicit = full_data_explicit.merge(users_df[['User-ID', 'Age']], on='User-ID') # Tambahkan age jika diperlukan

print("\nFirst 5 rows of merged explicit ratings data:")
print(full_data_explicit.head())

"""Ini memberikan gambaran awal tentang data rating buku yang digunakan, menunjukkan bahwa dari total 1,149,780 original ratings, hanya 433,671 yang merupakan explicit ratings yang dapat digunakan untuk analisis atau pelatihan model yang membutuhkan nilai rating spesifik. Tampilan beberapa baris pertama dari data explicit ratings yang sudah digabungkan memperlihatkan struktur datanya, meliputi User-ID, ISBN, nilai Book-Rating, judul buku (Book-Title), dan usia pengguna (Age), menegaskan ketersediaan informasi penting ini untuk memahami perilaku pengguna dalam memberikan rating dan karakteristik demografi mereka terkait dengan buku yang dinilai."""

# --- Filtering untuk Collaborative Filtering (mengurangi sparsity) ---
# Pertimbangkan pengguna yang memberi setidaknya N rating dan buku yang menerima setidaknya M rating
min_ratings_user = 5 # Contoh threshold
user_counts = full_data_explicit['User-ID'].value_counts()
active_users = user_counts[user_counts >= min_ratings_user].index
filtered_ratings_cf = full_data_explicit[full_data_explicit['User-ID'].isin(active_users)]

min_ratings_book = 5 # Contoh threshold
book_counts = filtered_ratings_cf['ISBN'].value_counts()
popular_books = book_counts[book_counts >= min_ratings_book].index
filtered_ratings_cf = filtered_ratings_cf[filtered_ratings_cf['ISBN'].isin(popular_books)]

print(f"\nShape of filtered_ratings_cf for Collaborative Filtering: {filtered_ratings_cf.shape}")
if filtered_ratings_cf.empty:
    print("WARNING: filtered_ratings_cf is empty. Thresholds for filtering might be too high or data is too sparse.")
else:
    print("Filtered ratings for CF are ready.")

"""mengindikasikan bahwa proses pemfilteran data rating telah berhasil diselesaikan untuk keperluan Collaborative Filtering, menghasilkan dataset siap pakai yang diberi nama filtered_ratings_cf. Dataset ini memiliki dimensi (135310, 5), yang berarti terdiri dari 135.310 baris data (kemungkinan merepresentasikan interaksi pengguna-buku berupa rating) dengan 5 kolom atribut (seperti User-ID, ISBN, Rating, dll.), yang kini siap digunakan untuk melatih atau mengaplikasikan model rekomendasi berbasis kolaborasi

# Modeling  Content-Based Filtering
"""

# --- Content-Based Filtering ---
from sklearn.metrics.pairwise import linear_kernel # Atau cosine_similarity jika mau
sample_fraction = 0.1 # Ambil 10% data (sekitar 27rb buku), sesuaikan sesuai kebutuhan/kemampuan RAM
# Atau gunakan jumlah absolut: sample_size = 20000
print(f"Taking a sample of {sample_fraction*100}% of the books for faster processing...")
books_cb_sample = books_df.sample(frac=sample_fraction, random_state=42).copy()
# -----------------------------------------------------

# Gunakan 'books_cb_sample' di langkah selanjutnya
books_cb_sample.fillna('', inplace=True)
books_cb_sample['content'] = books_cb_sample['Book-Title'] + ' ' + books_cb_sample['Book-Author'] + ' ' + books_cb_sample['Publisher']

# Inisialisasi TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2))

# Membuat matriks TF-IDF dari SAMPEL
print("Fitting TF-IDF on sample...")
tfidf_matrix_sample = tfidf.fit_transform(books_cb_sample['content'])
print("Shape of TF-IDF matrix (sample):", tfidf_matrix_sample.shape)

# Menghitung KEMIRIPAN pada SAMPEL
print("Calculating similarity on sample using linear_kernel...")
cosine_sim_cb_sample = linear_kernel(tfidf_matrix_sample, tfidf_matrix_sample)
# Atau: cosine_sim_cb_sample = cosine_similarity(tfidf_matrix_sample, tfidf_matrix_sample)
print("Shape of Similarity matrix (sample):", cosine_sim_cb_sample.shape)

# Membuat mapping dari ISBN ke Indeks untuk SAMPEL
books_cb_sample.reset_index(drop=True, inplace=True) # Reset index sample
indices_cb_sample = pd.Series(books_cb_sample.index, index=books_cb_sample['ISBN']).drop_duplicates()

# Fungsi untuk mendapatkan rekomendasi (disesuaikan untuk sampel)
def get_content_based_recommendations_sample(isbn, N=10):
    if isbn not in indices_cb_sample:
        return f"ISBN {isbn} not found in the sample's unique ISBN list."

    try:
      idx = indices_cb_sample[isbn]
      if isinstance(idx, pd.Series): idx = idx.iloc[0]
    except KeyError:
        return f"ISBN {isbn} not found in the sample indices mapping."
    except Exception as e:
        return f"Error finding index for sample ISBN {isbn}: {e}"

    sim_scores = list(enumerate(cosine_sim_cb_sample[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:N+1]
    book_indices = [i[0] for i in sim_scores]

    # Kembalikan N buku paling mirip dari SAMPEL
    recommended_books = books_cb_sample.iloc[book_indices][['ISBN', 'Book-Title', 'Book-Author']]
    recommended_books['similarity_score'] = [s[1] for s in sim_scores]
    return recommended_books

# Contoh penggunaan Content-Based Filtering pada SAMPEL
if not books_cb_sample.empty:
    # Cari ISBN contoh yang ADA di dalam sampel
    try:
        sample_isbn_cb_in_sample = books_cb_sample['ISBN'].iloc[0]
        sample_title_in_sample = books_cb_sample.loc[books_cb_sample['ISBN'] == sample_isbn_cb_in_sample, 'Book-Title'].values[0]
        print(f"\nRecommendations for book (from sample) with ISBN: {sample_isbn_cb_in_sample} ('{sample_title_in_sample}')")
        recommendations_cb_s = get_content_based_recommendations_sample(sample_isbn_cb_in_sample, N=5)
        print(recommendations_cb_s)
    except IndexError:
        print("\nSample dataframe is empty, cannot get sample ISBN.")
else:
    print("books_cb_sample dataframe is empty. Cannot provide recommendations.")

"""Output ini menunjukkan proses dan hasil dari sistem rekomendasi berbasis konten (content-based recommendation system) yang diterapkan pada dataset buku, khususnya untuk merekomendasikan buku-buku yang mirip dengan buku berjudul "'The F Word'". Sistem ini menggunakan sampel 10% dari data buku, menghitung kesamaan antar buku menggunakan representasi TF-IDF dan kernel linear, kemudian menghasilkan daftar buku rekomendasi bersama dengan skor kesamaan. Hasilnya menunjukkan beberapa buku yang direkomendasikan beserta detailnya, dengan skor kesamaan yang bervariasi, mencerminkan seberapa mirip konten buku-buku tersebut berdasarkan analisis tekstual oleh sistem.

# Modeling Collaborative Filtering
"""

# --- Collaborative Filtering ---
# Menggunakan filtered_ratings_cf yang sudah dipersiapkan
if filtered_ratings_cf.empty:
    print("Skipping Collaborative Filtering due to empty filtered_ratings_cf.")
else:
    # Persiapan data untuk Surprise
    reader = Reader(rating_scale=(1, 10)) # Rating eksplisit dari 1 hingga 10
    data_cf = Dataset.load_from_df(filtered_ratings_cf[['User-ID', 'ISBN', 'Book-Rating']], reader)

    # Split data menjadi train dan test set
    trainset_cf, testset_cf = surprise_train_test_split(data_cf, test_size=0.2, random_state=42)

    # Menggunakan SVD
    algo_svd = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42) # Parameter bisa di-tune
    print("\nTraining SVD model...")
    algo_svd.fit(trainset_cf)
    print("SVD model trained.")

    # Membuat prediksi pada test set
    predictions_svd = algo_svd.test(testset_cf)

    # Evaluasi model (RMSE dan MAE)
    rmse_svd = accuracy.rmse(predictions_svd)
    mae_svd = accuracy.mae(predictions_svd)
    print(f"SVD - RMSE: {rmse_svd}")
    print(f"SVD - MAE: {mae_svd}")

    # Fungsi untuk mendapatkan top-N rekomendasi untuk pengguna
    def get_collaborative_filtering_recommendations(user_id, N=10):
        # Dapatkan daftar semua ISBN yang belum dirating oleh pengguna
        rated_isbns = filtered_ratings_cf[filtered_ratings_cf['User-ID'] == user_id]['ISBN'].unique()
        all_isbns = filtered_ratings_cf['ISBN'].unique()
        unrated_isbns = [isbn for isbn in all_isbns if isbn not in rated_isbns]

        if not unrated_isbns:
            return "User has rated all available books or no unrated books found for this user in the filtered set."

        # Prediksi rating untuk buku yang belum dirating
        test_set_for_user = [[user_id, isbn, 0] for isbn in unrated_isbns] # 0 adalah placeholder
        predictions = algo_svd.test(test_set_for_user)

        predictions.sort(key=lambda x: x.est, reverse=True) # Urutkan berdasarkan estimasi rating

        top_n_predictions = predictions[:N]

        # Dapatkan detail buku untuk rekomendasi
        recommended_book_isbns = [pred.iid for pred in top_n_predictions]
        recommended_book_details = books_df[books_df['ISBN'].isin(recommended_book_isbns)][['ISBN', 'Book-Title', 'Book-Author']]

        # Tambahkan estimasi rating
        # Perlu mencocokkan kembali karena urutan bisa berubah jika ada ISBN duplikat atau tidak ditemukan
        est_ratings_map = {pred.iid: pred.est for pred in top_n_predictions}
        recommended_book_details['estimated_rating'] = recommended_book_details['ISBN'].map(est_ratings_map)

        # Urutkan berdasarkan estimasi rating lagi setelah merge
        recommended_book_details = recommended_book_details.sort_values(by='estimated_rating', ascending=False).reset_index(drop=True)

        return recommended_book_details

    # Contoh penggunaan Collaborative Filtering
    if not filtered_ratings_cf.empty:
        # Ambil User-ID contoh dari dataset yang difilter
        sample_user_id_cf = filtered_ratings_cf['User-ID'].unique()[0]
        print(f"\nRecommendations for User-ID: {sample_user_id_cf} using SVD")
        recommendations_cf = get_collaborative_filtering_recommendations(sample_user_id_cf, N=5)
        print(recommendations_cf)
    else:
        print("filtered_ratings_cf is empty. Cannot provide CF recommendations.")

"""Menampilkan hasil dari penerapan model Singular Value Decomposition (SVD) untuk sistem rekomendasi, kemungkinan besar menggunakan pendekatan collaborative filtering. Setelah model SVD dilatih, kinerjanya dievaluasi menggunakan metrik RMSE sebesar 1.574 dan MAE sebesar 1.210, yang mengindikasikan tingkat kesalahan prediksi rating. Selanjutnya, sistem ini memberikan rekomendasi buku yang dipersonalisasi untuk Pengguna dengan ID 276747, menyajikan daftar buku beserta perkiraan rating (estimated_rating) yang menunjukkan seberapa besar kemungkinan pengguna tersebut akan menyukai buku-buku yang direkomendasikan berdasarkan pola rating dari pengguna lain.

# Evaluation Content-Based Filtering
"""

# --- Evaluasi Content-Based Filtering (Sangat Sederhana - Inspeksi Visual) ---

print("\n--- Evaluasi Content-Based Filtering (Sangat Sederhana) ---")

# Catatan: Ini adalah inspeksi visual/kualitatif.
# Anda perlu melihat hasilnya dan menilai sendiri relevansinya.

if not books_cb_sample.empty:
    # Pilih satu buku acak dari sampel untuk diuji
    try:
        random_book_in_sample = books_cb_sample.sample(1, random_state=42).iloc[0]
        isbn_to_test_simple = random_book_in_sample['ISBN']
        title_to_test_simple = random_book_in_sample['Book-Title']

        print(f"\nMeminta rekomendasi untuk buku (dari sampel):")
        print(f"ISBN: {isbn_to_test_simple}")
        print(f"Judul: '{title_to_test_simple}'")

        # Dapatkan rekomendasi menggunakan fungsi yang sudah ada
        recommendations_simple = get_content_based_recommendations_sample(isbn_to_test_simple, N=10)

        # Tampilkan hasil rekomendasi
        if isinstance(recommendations_simple, pd.DataFrame):
            print("\nRekomendasi Teratas:")
            print(recommendations_simple)
            print("\n(Periksa apakah buku-buku di atas terlihat mirip dengan buku input)")
        else:
            # Menangani kasus jika ISBN tidak ditemukan di sampel
            print(recommendations_simple)

    except Exception as e:
        print(f"Terjadi error saat mencoba mendapatkan rekomendasi untuk sampel acak: {e}")

else:
    print("Dataframe sample (books_cb_sample) kosong, tidak bisa melakukan evaluasi sederhana.")

# --- Akhir Bagian Evaluasi Sederhana ---

"""Menyajikan evaluasi dari sistem content-based filtering yang sederhana, di mana sistem memberikan rekomendasi buku berdasarkan kesamaan konten dengan buku input "Enterprise One to One: Tools for Competing in the Interactive Age". Hasilnya menampilkan daftar buku yang direkomendasikan beserta skor kesamaan mereka. Meskipun beberapa judul yang direkomendasikan seperti "One to One B2B" dan "The One to One Fieldbook" secara tematis terlihat terkait, skor kesamaan yang relatif rendah (di bawah 0.3) untuk sebagian besar rekomendasi menunjukkan bahwa metode analisis konten atau perhitungan kesamaan yang digunakan mungkin perlu ditingkatkan untuk menghasilkan rekomendasi yang lebih relevan atau serupa secara kuat.

# Evaluation Collaborative Filtering
"""

# Hasil evaluasi SVD sudah dicetak sebelumnya saat training
if 'rmse_svd' in globals() and 'mae_svd' in globals():
    print("\n--- Evaluation Metrics for Collaborative Filtering (SVD) ---")
    print(f"SVD Model - RMSE: {rmse_svd:.4f}")
    print(f"SVD Model - MAE: {mae_svd:.4f}")
    print("\nInterpretasi:")
    print(f"RMSE sebesar {rmse_svd:.4f} menunjukkan bahwa rata-rata, prediksi rating model SVD kita menyimpang sekitar {rmse_svd:.4f} poin rating dari rating sebenarnya, dengan kesalahan yang lebih besar diberi bobot lebih.")
    print(f"MAE sebesar {mae_svd:.4f} menunjukkan bahwa rata-rata, prediksi rating model SVD kita menyimpang sekitar {mae_svd:.4f} poin rating dari rating sebenarnya.")
    print("Nilai ini perlu dipertimbangkan dalam konteks skala rating (1-10). Semakin rendah, semakin baik performa model dalam memprediksi rating.")
else:
    print("\nSkipping CF evaluation as SVD model results are not available (likely due to empty filtered_ratings_cf).")

"""
Menyajikan metrik evaluasi untuk model Collaborative Filtering menggunakan metode SVD, dengan nilai RMSE sebesar 1.5741 dan MAE sebesar 1.2101. Nilai-nilai ini mengindikasikan performa model dalam memprediksi rating pengguna; secara rata-rata, prediksi rating model SVD menyimpang sekitar 1.5741 poin dari rating sebenarnya (dengan penalti lebih besar untuk kesalahan besar) dan memiliki rata-rata selisih absolut 1.2101 poin dari rating sebenarnya. Dalam konteks skala rating 1-10, semakin rendah nilai RMSE dan MAE, semakin baik kemampuan model dalam memprediksi rating, sehingga nilai-nilai ini memberikan gambaran kuantitatif tentang akurasi rekomendasi yang dihasilkan oleh model SVD ini."""